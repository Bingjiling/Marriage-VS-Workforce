<<<<<<< HEAD
?predict.randomForest
?predict.randomForest
predict.randomForest(fit1, newdata = Math)
fit1_pred <- predict(fit1, newdata = Math)
fit1 <- randomForest(x = predictors_train, y = Language$G3, importance = TRUE,
xtest = predictors_test, ytest = Math$G3)
fit1_pred <- predict(fit1, newdata = Math)
class(fit1)
fit1
fit1$test$mse[494:500]
rm(fit1)
?normalizeData
head(combi)
?randomForest
head(Math[,1:32])
set.seed(4058)
fit1 <- randomForest(G3 ~ . - G1 - G2, data = Language, importance = TRUE,
xtest = Math[, 1:30], ytest = Math$G3)
fit1
fit1$test$mse
library(gbm)
?gbm
?predict.gbm
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 500, interaction.depth = 4, shrinkage = 0.001)
fit2_pred <- gbm(fit2, newdata = Math, n.trees = 500, type = "response")
fit2_pred <- predict(fit2, newdata = Math, n.trees = 500, type = "response")
avg_sq_error(fit2_pred, Math$G3)
avg_sq_error <- function(pred_values, actual_values) {
return(mean((actual_values - pred_values) ** 2))
}
avg_sq_error(fit2_pred, Math$G3)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 4, shrinkage = 0.01)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 4, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 500, type = "response")
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 4, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 1, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 4, shrinkage = 0.001)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 6, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 10, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 3000, interaction.depth = 10, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 3000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 12, shrinkage = 0.01)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 3000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 12, shrinkage = 0.1)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
avg_sq_error(predict(fit2, n.trees = 2000, type = "response"), Language$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 2000, interaction.depth = 4, shrinkage = 0.1)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 2000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
avg_sq_error(predict(fit2, n.trees = 2000, type = "response"), Language$G3)
set.seed(4058)
fit2 <- gbm(G3 ~ . - G1 - G2, distribution = "gaussian", data = Language,
n.trees = 1000, interaction.depth = 4, shrinkage = 0.1)
fit2_pred <- predict(fit2, newdata = Math, n.trees = 1000, type = "response")
avg_sq_error(fit2_pred, Math$G3)
library(RSNNS)
?elman
-0.140 + 0.097
+ 3.977 - 1.596
plot.new()
abline(a = 3.977, b = -0.140, col = "red")
dev.off()
plot(c(-10, 10), c(-10, 10), type = "n", xlab = "x", ylab = "y", asp = 1)
abline(a = 3.977, b = -0.140, col = "red")
abline(a = 2.381, b = -0.043, col = "green")
?plot
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week")
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week", asp = 1)
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week")
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week", asp = 1)
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week")
abline(a = 3.977, b = -0.140, col = "red")
abline(a = 2.381, b = -0.043, col = "green")
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week", asp = 1)
abline(a = 3.977, b = -0.140, col = "red")
abline(a = 2.381, b = -0.043, col = "green")
plot(c(0, 20), c(0, 7), type = "n", xlab = "Years of Schooling", ylab = "Lonely Days per Week")
abline(a = 3.977, b = -0.140, col = "red")
abline(a = 2.381, b = -0.043, col = "green")
abline(v = 0, col = "black")
?exp
exp(0)
exp(exp())
exp(10)
exp(1)
exp(exp(1))
log(exp(1))
exp(-0.33)
exp(0.98)
1-0.68
library(ISLR)
data("Caravan", package = "ISLR")
str(Caravan)
testing <- Caravan[1:1000, ]
training <- Caravan[1001:5822, ]
library(bartMachine)
install.packages("rJava")
library(bartMachine)
library(bartMachine)
library(bartMachine)
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
Sys.setenv(JAVA_HOME="C:\Program Files\Java\jre1.8.0_66")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_66")
library(rJava)
library(bartMachine)
library(ISLR)
data("Caravan", package = "ISLR")
str(Caravan)
testing <- Caravan[1:1000, ]
training <- Caravan[1001:5822, ]
set_bart_machine_num_cores(parallel::detectCores())
bart <- bartMachine(X = training[,1:85], y = training$Purchase)
library(bartMachine)
options(java.parameters = "-Xmx2500m")
library(ISLR)
data("Caravan", package = "ISLR")
str(Caravan)
testing <- Caravan[1:1000, ]
training <- Caravan[1001:5822, ]
set_bart_machine_num_cores(parallel::detectCores())
options(java.parameters = "-Xmx2500m")
bart <- bartMachine(X = training[,1:85], y = training$Purchase)
bart <- bartMachine(X = training[,1:85], y = training$Purchase,
mem_cache_for_speed = FALSE)
bart
?predict.bart
bart_pred <- predict(bart, new_data = testing[, 1:85], type = "class")
table(testing$Purchase, bart_pred)
table(testing$Purchase)
table(bart_pred)
class(training$Purchase)
summary(training$Purchase)
bart_pred_train <- predict(bart, type = "class")
bart_pred_train <- predict(bart, new_data = training[, 1:85], type = "class")
tale(training$Purchase, bart_pred_train)
table(training$Purchase, bart_pred_train)
bart
rm(bart_pred_train)
library(rminer)
?fit
rpart_pred <- fit(Purchase ~ ., data = training, model = "rpart")
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart")
set.seed(4058)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart")
?predict.fit
?predict.bartMachine
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
table(testing$Purchase, rpart_pred)
rpart_pred[1:50]
rpart_pred[1:50, 1:2]
rpart_pred[500:550, 1:2]
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart")
rpart_pred <- predict(rpart_fit, newdata = testing)
rm(rpart_fit, rpart_pred)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart")
str(rpart_fit)
rpart_pred <- predict(rpart_fit, newdata = testing)
?fit
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", method = "class")
rpart_pred <- predict(rpart_fit, newdata = testing)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", method = "class",
control = rpart.control(cp = 0.05))
library(rpart)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", method = "class",
control = rpart.control(cp = 0.05))
rpart_pred <- predict(rpart_fit, newdata = testing)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", method = "class",
control = rpart.control(cp = 0.1))
rpart_pred <- predict(rpart_fit, newdata = testing)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", method = "class",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing)
summary(rpart_pred)
table(testing$Purchase, rpart_pred)
rpart_pred[1:50, 1:2]
?apply
max(0.7, 0.3)
rpart_pred2 <- apply(rpart_pred, MARGIN = 1, max)
?predict.rpart
rm(rpart_pred2)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing, type = "class"")
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing, type = "class")
class(rpart_pred)
rm(rpart_fit, rpart_pred)
?predict.fit
?fit
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class"
control = rpart.control(cp = 0.001))
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class",
control = rpart.control(cp = 0.01))
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class",
control = rpart.control(cp = 0.0001))
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
table(testing$Purchase, bart_pred)
?bartMachine
bart <- bartMachine(X = training[, 1:85], y = training$Purchase,
mem_cache_for_speed = FALSE, prob_rule_class = 0.25)
bart
table(testing$Purchase, bart_pred)
table(testing$Purchase, bart_pred)
bart
bart <- bartMachine(X = training[, 1:85], y = training$Purchase,
mem_cache_for_speed = FALSE, prob_rule_class = 0.5)
bart
table(testing$Purchase, bart_pred)
set.seed(4058)
rpart_fit <- fit(Purchase ~ ., data = training, model = "rpart", task = "class",
control = rpart.control(cp = 0.001))
rpart_pred <- predict(rpart_fit, newdata = testing)
table(testing$Purchase, rpart_pred)
table(testing$Purchase)
install.packages("rJava")
library(rJava)
Sys.getenv("JAVA_HOME")
Sys.getenv("JAVA_HOME")
install.packages("rJava")
system("java -version")
install.packages('rJava', .libPaths()[1], 'http://www.rforge.net/')
install.packages("rJava")
library(rJava)
?rJava
??rJava
Sys.getenv("R_ARCH")
Sys.getenv("JAVA")
options(java.parameters = "-Xmx6g") # set 6GB of memory aside for the Java heap space for bartMachine()
?mice
library(mice)
?mice
options(java.parameters = "-Xmx6g") # set 6GB of memory aside for the Java heap space for bartMachine()
130292355 / 222474111
install.packages("installr")
?installr
library(installr)
?installr
installr()
lm_1 <- lm(eval ~ beauty)
lm_2 <- lm(eval ~ beauty + female)
display(lm_2)
library("arm")
display(lm_2)
lm_2 <- lm(eval ~ beauty + female)
Sys.getenv("PATH")
library(QMSS)
?QMSS
?propOddsTest
propOddsTest
predProb
?do.call
Sys.getenv("PATH")
system("g++ -v")
system('where make')
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXXFLAGS=-O3 -Wno-unused-variable -Wno-unused-function -Wno-unused-local-typedefs",
file = M, sep = "\n", append = TRUE)
install.packages("rstan", dependencies = TRUE)
retrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){
z <- qt(1-alpha/2, df)
p.hi <- 1 - pt(z-A/s, df)
p.lo <- pt(-z-A/s, df)
power <- p.hi + p.lo
typeS <- p.lo/power
estimate <- A + s*rt(n.sims,df)
significant <- abs(estimate) > s*z
exaggeration <- mean(abs(estimate)[significant])/A
return(list(power=power, typeS=typeS, exaggeration=exaggeration))
}
retrodesign(0.1, 1)
retrodesign(0.1, 0.1)
retrodesign(0.1, 0.001)
retrodesign(0.1, 0.01)
retrodesign(0.1, 0.09)
retrodesign(0.1, 0.05)
[:alpha:]
[[:alpha:]]
?getGroupMembers
?alpha
?alphabet
letters
?letters
letters
?sample
sample(letters, 5)
sample.int(letters, 5)
sample.int(10, 1)
sample.int(10, 9)
sample(letters, 5)
sample(letters, 5)
sample(letters, 5)
sample(letters, 5)
sample(letters, 5)
retrodesign(1, 2.3)
retrodesign(5, 2.3)
retrodesign(3, 2.3)
install.packages("maps")
install.packages("maptools")
install.packages("rgdal")
install.packages("RColorBrewer")
install.packages("classInt")
library(maps)
library(maptools)
library(rgdal)
library(RColorBrewer)
library(classInt)
?classInt
curve(0.50 + (350000)/x)
curve(1 / (0.50 + (350000)/x))
?curve
curve(1 / (0.50 + (350000)/x), from = 0, to = 5000000)
curve(1 / (0.20 + (750000)/x), from = 0, to = 5000000)
1 / (0.50 + 350000)
plot()
plot.new()
curve(1 / (0.50 + (350000)/x), from = 0, to = 5000000, col = "red")
curve(1 / (0.20 + (750000)/x), from = 0, to = 5000000, col = "blue")
curve(1 / (0.50 + (350000)/x), from = 0, to = 5000000, col = "red")
curve(1 / (0.20 + (750000)/x), from = 0, to = 5000000, col = "blue", add = TRUE)
A = (1 / (0.50 + (350000/2000000)))
B = (1 / (0.20 + (750000/2000000)))
750000 + 400000
1150000 - 500000 - 250000
400000 / 0.20
1150000 - 100000 - 250000
800000 / 0.50
160000 / 1150000
(1 / 0.50 + (350000 / 160000))
1600000 / 1150000
(1 / ((0.50 * 1600000) + (350000/1600000)))
0.5 * sqrt(6 / 166000)
runif(n = 1000)
uniform_dist = runif(1000, min = 5, max = 10)
hist(uniform_dist)
uniform_dist = runif(100000, min = 5, max = 10)
hist(uniform_dist)
hist(uniform_dist, bins = 30)
?hist
hist(uniform_dist, breaks = 200)
norm_dist <- rnorm(100000)
hist(norm_dist)
hist(norm_dist, breaks = 100)
uniform_dist
install.packages("shiny")
library(shiny::runGitHub("bgoodri/shiny_beta2"))
library(shiny)
shiny::runGitHub("bgoodri/shiny_beta2")
shiny::runGitHub("bgoodri/shiny_beta2")
library(repmis)
?source_DropboxData
?unif
?runit
?runif
?Uniform
?integrate
uniftest <- function(alpha, beta) {
1 / (beta - alpha)
}
uniftest(5, 10)
integrate(uniftest, 5, 10)
integrate(uniftest(5, 10), 5, 10)
integrate(uniftest)
integrate(uniftest, 5, 10)
integrate(uniftest(5), 5, 10)
integrate(uniftest(5, 10), 5, 10)
uniftest <- function(x, alpha, beta) {
ifelse(x >= alpha | x <= beta, 1 / (beta - alpha), 0)
}
uniftest(6, 5, 10)
uniftest(4.9, 5, 10)
uniftest <- function(x, alpha, beta) {
ifelse(x >= alpha & x <= beta, 1 / (beta - alpha), 0)
}
uniftest(4.9, 5, 10)
uniftest(8, 5, 10)
integrate(uniftest, 5, 10)
integrate(uniftest(6, 5, 10), 5, 10)
integrate(uniftest(6, 5, 10), -Inf, Inf)
integrate(uniftest, -Inf, Inf)
integrate(uniftest(alpha = 5, beta = 10), -Inf, Inf)
integrate(uniftest(x = 0:10, alpha = 5, beta = 10), -Inf, Inf)
integrate(uniftest, lower = -Inf, upper = Inf, alpha = 5, beta = 10)
integrate(uniftest, lower = -Inf, upper = Inf, alpha = 0, beta = 1)
yuniftest <- function(y) {
ifelse(y >= 0 & y <= 1, (0.5) ^ (-0.5))
}
integrate(yuniftest, lower = -Inf, upper = Inf)
yuniftest <- function(y) {
ifelse(y >= 0 & y <= 1, (0.5) ^ (-0.5), 0)
}
integrate(yuniftest, lower = -Inf, upper = Inf)
integrate(yuniftest, lower = 0, upper = 1)
yuniftest <- function(y) {
1 / (2 * sqrt(y))
}
integrate(yuniftest, lower = 0, upper - 1)
integrate(yuniftest, lower = 0, upper = 1)
?yuniftest
rm(yuniftest)
Y <- function(y) {
1 / (2 * sqrt(y))
}
integrate(Y, 0, 1)
Z <- function(z) {
2 * z
}
integrate(Z, 0, 1)
rm(uniftest, Y, Z)
Y_PDF <- function(y) {
1 / (2 * sqrt(y))
}
integrate(Y_PDF, 0, 1)
Z_PDF <- function(z) {
2 * z
}
integrate(Z_PDF, 0, 1)
B_PDF <- function(beta, m, alpha) {
(alpha * (m ^ alpha)) / (beta ^ (alpha + 1))
}
integrate(B_PDF, lower = 0, upper = Inf, m = 2, alpha = 2)
integrate(B_PDF, lower = 0, upper = Inf, m = 5, alpha = 2)
integrate(B_PDF, lower = 0, upper = Inf, m = 5, alpha = 1)
integrate(B_PDF, lower = 0, upper = Inf, m = 1, alpha = 1)
B_PDF <- function(beta, m, alpha) {
ifelse(beta > m & m > 0 & alpha > 0, (alpha * (m ^ alpha)) / (beta ^ (alpha + 1)), 0)
}
integrate(B_PDF, lower = 0, upper = Inf, m = 1, alpha = 1)
integrate(B_PDF, lower = 0, upper = Inf, m = 3, alpha = 1)
integrate(B_PDF, lower = 0, upper = Inf, m = 6, alpha = 2)
integrate(B_PDF, lower = 0, upper = Inf, m = 3, alpha = 2)
?rnorm
?rint
?random
?Random
integrate(B_PDF, lower = 0, upper = Inf, m = 1, alpha = 2)
B_X_JPDF <- function(x, beta, m, alpha) {
ifelse(beta > m & m > 0 & alpha > 0, (alpha * (m ^ alpha)) / (beta ^ (alpha + 2)), 0)
}
integrate(B_X_JPDF, lower = 0, upper = Inf, beta = 2, m = 1, alpha = 2)
integrate(B_X_JPDF, lower = 0, upper = Inf, beta = 2, m = 1)
integrate(B_X_JPDF, lower = 0, upper = Inf, beta = 2, m = 1, alpha = 1)
B_X_JPDF <- function(beta, m, alpha) {
ifelse(beta > m & m > 0 & alpha > 0, (alpha * (m ^ alpha)) / (beta ^ (alpha + 2)), 0)
}
integrate(B_X_JPDF, lower = 0, upper = Inf, m = 1, alpha = 1)
integrate(B_X_JPDF, lower = 0, upper = Inf, m = 1, alpha = 2)
rm(B_X_JPDF)
X_PDF <- function(x, beta) {
ifelse(x >= 0 & x <= beta, 1 / beta, 0)
}
integrate(X_PDF, -Inf, Inf, beta = 5)
integrate(X_PDF, -Inf, Inf, beta = 5)$3
class(integrate(X_PDF, -Inf, Inf, 5))
class(integrate(X_PDF, -Inf, Inf, 5))$value
integrate(X_PDF, -Inf, Inf, beta = 5)$value
rm(X_PDF)
integrate(B_PDF, lower = 0, upper = Inf, m = 2, alpha = 2)
setwd("C:/Users/Arnold/Dropbox/Columbia/2016 Spring/STAT W4249 Applied Data Science/cycle1-7")
getwd()
=======
Y1 = sum(X1*Vh)-c
Y2 = sum(X2*Vh)-c
##Problem 2
#' 1
#' function classify(S,z)
#' @ z the Perceptron weight vector
#' @ S sample data set
#' @ return class label vector y
classify = function(S,z){
a = matrix(z,ncol=1)
sign(S %*% z)
}
#' 2
#' function perceptrain(S,y)
#' returns a list containing z and Z history, where z is again
#' the normal vector zH of the hyperplane
perceptrain = function(S,y){
z = rep(100,ncol(S))
outcome = z
k = 1
while (sum(abs(classify(S,z)-y))!=0){
indicator = abs(classify(S,z)-y)/2
indicator = as.vector(indicator)
gradient = colSums(S*(indicator*(-y)))
z = z - 1/k*gradient
k = k+1
outcome = cbind(z,outcome)
}
return(outcome)
}
#' For 3D vector (-1028,3,92)
#' generate data and train the perceptron
trainData = fakedata(c(-1028,3,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
#' plot the the training data and the trajectory
#' of the algorithm
#' Add color
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
# add the train lines
for(i in 1:ncol(zMatrix)){
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
}
#' Generate test data
#' test the classifier
testData = fakedata(c(-1028,3,92),100)
errorRate = sum(classify(testData$S,classifier)-testData$y)/length(testData$y)
# plot the test data and the classifier
testData$color = rep("Black",length(testData$y))
testData$color[testData$y==-1] = "Yellow"
plot(testData$S[,1],testData$S[,2],col = testData$color)
i = 2
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
trainData = fakedata(c(-1028,3,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
##Problem 1
Vh = c(1/sqrt(2),1/sqrt(2))
c = 1/(2*sqrt(2))
X1 = c(-3,0)
X2 = c(1/2,1/2)
Y1 = sum(X1*Vh)-c
Y2 = sum(X2*Vh)-c
##Problem 2
#' 1
#' function classify(S,z)
#' @ z the Perceptron weight vector
#' @ S sample data set
#' @ return class label vector y
classify = function(S,z){
a = matrix(z,ncol=1)
sign(S %*% z)
}
#' 2
#' function perceptrain(S,y)
#' returns a list containing z and Z history, where z is again
#' the normal vector zH of the hyperplane
perceptrain = function(S,y){
z = rep(100,ncol(S))
outcome = z
k = 1
while (sum(abs(classify(S,z)-y))!=0){
indicator = abs(classify(S,z)-y)/2
indicator = as.vector(indicator)
gradient = colSums(S*(indicator*(-y)))
z = z - 1/k*gradient
k = k+1
outcome = cbind(z,outcome)
}
return(outcome)
}
#' For 3D vector (-1028,3,92)
#' generate data and train the perceptron
trainData = fakedata(c(-1028,3,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
#' plot the the training data and the trajectory
#' of the algorithm
#' Add color
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
# add the train lines
for(i in 1:ncol(zMatrix)){
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
}
#' Generate test data
#' test the classifier
testData = fakedata(c(-1028,3,92),100)
errorRate = sum(classify(testData$S,classifier)-testData$y)/length(testData$y)
# plot the test data and the classifier
testData$color = rep("Black",length(testData$y))
testData$color[testData$y==-1] = "Yellow"
plot(testData$S[,1],testData$S[,2],col = testData$color)
i = 2
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
zMatrix
i = 1
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
plot(1:7,2:8)
abline(-1,3)
abline(1,3)
##Problem 1
Vh = c(1/sqrt(2),1/sqrt(2))
c = 1/(2*sqrt(2))
X1 = c(-3,0)
X2 = c(1/2,1/2)
Y1 = sum(X1*Vh)-c
Y2 = sum(X2*Vh)-c
##Problem 2
#' 1
#' function classify(S,z)
#' @ z the Perceptron weight vector
#' @ S sample data set
#' @ return class label vector y
classify = function(S,z){
a = matrix(z,ncol=1)
sign(S %*% z)
}
#' 2
#' function perceptrain(S,y)
#' returns a list containing z and Z history, where z is again
#' the normal vector zH of the hyperplane
perceptrain = function(S,y){
z = rep(100,ncol(S))
outcome = z
k = 1
while (sum(abs(classify(S,z)-y))!=0){
indicator = abs(classify(S,z)-y)/2
indicator = as.vector(indicator)
gradient = colSums(S*(indicator*(-y)))
z = z - 1/k*gradient
k = k+1
outcome = cbind(z,outcome)
}
return(outcome)
}
#' For 3D vector (-1028,3,92)
#' generate data and train the perceptron
trainData = fakedata(c(-1028,3,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
#' plot the the training data and the trajectory
#' of the algorithm
#' Add color
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
# add the train lines
for(i in 1:ncol(zMatrix)){
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
}
#' Generate test data
#' test the classifier
testData = fakedata(c(-1028,3,92),100)
errorRate = sum(classify(testData$S,classifier)-testData$y)/length(testData$y)
# plot the test data and the classifier
testData$color = rep("Black",length(testData$y))
testData$color[testData$y==-1] = "Yellow"
plot(testData$S[,1],testData$S[,2],col = testData$color)
i = 2
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
i=1
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
i=2
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
i=1
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
i=2
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
trainData = fakedata(c(-28,700,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
dim(zMatrix)
trainData = fakedata(c(-28,700,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
trainData = fakedata(c(-28,700,92),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
dim(zMatrix)
class(zMatrix)
zMatrix
trainData = fakedata(c(-1963,794,8),100)
zMatrix = perceptrain(trainData$S, trainData$y)
classifier = zMatrix[,1]
trainData$color = rep("Black",length(trainData$y))
trainData$color[trainData$y==-1] = "Yellow"
plot(trainData$S[,1],trainData$S[,2],col = trainData$color)
# add the train lines
for(i in 1:ncol(zMatrix)){
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(intercept,slope)
}
testData = fakedata(c(-1963,794,8),100)
errorRate = sum(classify(testData$S,classifier)-testData$y)/length(testData$y)
# plot the test data and the classifier
testData$color = rep("Black",length(testData$y))
testData$color[testData$y==-1] = "Yellow"
plot(testData$S[,1],testData$S[,2],col = testData$color)
i = 1
slope = -zMatrix[1,i]/zMatrix[2,i]
intercept = -zMatrix[3,i]/zMatrix[2,i]
abline(slope,intercept)
errorRate
zMatrix
?plot
## a
x = (0:80)/20
y = exp(-x)
plot(x, y, type = "l", main = "p(x;1)")
x = c(1, 2, 4)
y = exp(-x)
points(x, y, col="yel")
x = c(1, 2, 4)
y = exp(-x)
points(x, y, col="red")
y2 = 2*exp(-2*x)
L1 = prod(y)
L2 = prod(y2)
L1
L2
rm(list = ls())
x = -100:100
y = log(1+exp(x))
plot(x,y)
x = -100:100
y = log(1+exp(-x))
plot(x,y)
y
x = -10:10
y = log(1+exp(-x))
plot(x,y)
x = seq(-10, 10, 0.1)
y = log(1+exp(-x))
plot(x,y)
rm(list=ls())
library("data.table")
install.packages("data.table")
library("data.table")
library("ggplot2")
?save
popDataA = fread("/Users/yutou/Data Science/Data/2013-american-community-survey/pums/ss13pusa.csv", select = colsToKeep)
reRead = 1
if(reRead == 1){
colsToKeep = c("MAR", "ST", "PINCP","SCHL", "ESR")
popDataA = fread("/Users/yutou/Data Science/Data/2013-american-community-survey/pums/ss13pusa.csv", select = colsToKeep)
}
head(popDataA)
#'
#'Read in Dataset
#'@par:
#'colsToKeep : vector of strings / parameters to be kept
#'reRead     : Boolean
#'pathA      : string / path of "ss13pusa.csv"
#'pathB      : string / path of "ss13pusb.csv"
readIn = function(reRead, colsToKeep, pathA, pathB){
if(reRead == TRUE){
popDataA = fread(pathA, select = colsToKeep)
popDataA = fread(pathB, select = colsToKeep)
popData = rbind(popDataA, popDataB)
rm(popDataA, popDataB)
save(popData, file = "popData.RData")
}else{
load("popData.RData")
}
return(popData)
}
rm(list = ls())
#'
#'Read in Dataset
#'@par:
#'colsToKeep : vector of strings / parameters to be kept
#'reRead     : Boolean
#'pathA      : string / path of "ss13pusa.csv"
#'pathB      : string / path of "ss13pusb.csv"
readIn = function(reRead, colsToKeep, pathA, pathB){
if(reRead == TRUE){
popDataA = fread(pathA, select = colsToKeep)
popDataA = fread(pathB, select = colsToKeep)
popData = rbind(popDataA, popDataB)
rm(popDataA, popDataB)
save(popData, file = "popData.RData")
}else{
load("popData.RData")
}
return(popData)
}
rm(list=ls())
install.packages("coin")
library("coin")
class(group)
score = c(68, 77, 82, 85, 53, 64, 71)
group = c("A","A","A","A", "B","B","B")
class(group)
group = as.factor(group)
class(group)
?"independence_test"
score = c(68, 77, 82, 85, 53, 64, 71)
group = c("A","A","A","A", "B","B","B")
group = as.factor(group)
independence_test(score|group)
score = c(68, 77, 82, 85, 53, 64, 71)
group = c("A","A","A","A", "B","B","B")
group = as.factor(group)
f = as.formula(score|group)
independence_test(f)
score = c(68, 77, 82, 85, 53, 64, 71)
group = c("A","A","A","A", "B","B","B")
group = as.factor(group)
independence_test(score~group)
?sample
groupA = c(1.31,1.45,1.12,1.16,1.30,1.50,1.20,1.22,1.42,1.14,1.23,1.59,1.11,1.10,1.53,1.52,1.17,1.49,1.62,1.29)
groupB = c(1.13,1.71,1.39,1.15,1.33,1.00,1.03,1.68,1.76,1.55,1.34,1.47,1.74,1.74,1.19,1.15,1.20,1.59,1.47)
data = c(groupA, groupB)
disMean = vector()
for(i in 1:1000){
sam = sample(length(data),length(groupA))
samA = data[sam]
samB = data[-sam]
dis = mean(samA) - mean(samB)
disMean = c(disMean, dis)
}
head(disMean)
mean(groupA)-mean(groupB)
groupA = c(1.31,1.45,1.12,1.16,1.30,1.50,1.20,1.22,1.42,1.14,1.23,1.59,1.11,1.10,1.53,1.52,1.17,1.49,1.62,1.29)
groupB = c(1.13,1.71,1.39,1.15,1.33,1.00,1.03,1.68,1.76,1.55,1.34,1.47,1.74,1.74,1.19,1.15,1.20,1.59,1.47)
data = c(groupA, groupB)
disMean = vector()
#Generate 1000 samples
set.seed(7)
for(i in 1:1000){
sam = sample(length(data),length(groupA))
samA = data[sam]
samB = data[-sam]
dis = mean(samA) - mean(samB)
disMean = c(disMean, dis)
}
#Put the true mean diff in and see the index to find the p-value
testStatic = mean(groupA)-mean(groupB)
disMean = c(disMean,testStatic)
disMean = sort(disMean)
index = match(testStatistic, disMean)
groupA = c(1.31,1.45,1.12,1.16,1.30,1.50,1.20,1.22,1.42,1.14,1.23,1.59,1.11,1.10,1.53,1.52,1.17,1.49,1.62,1.29)
groupB = c(1.13,1.71,1.39,1.15,1.33,1.00,1.03,1.68,1.76,1.55,1.34,1.47,1.74,1.74,1.19,1.15,1.20,1.59,1.47)
data = c(groupA, groupB)
disMean = vector()
#Generate 1000 samples
set.seed(7)
for(i in 1:1000){
sam = sample(length(data),length(groupA))
samA = data[sam]
samB = data[-sam]
dis = mean(samA) - mean(samB)
disMean = c(disMean, dis)
}
#Put the true mean diff in and see the index to find the p-value
testStatic = mean(groupA)-mean(groupB)
disMean = c(disMean,testStatic)
disMean = sort(disMean)
index = match(testStatic, disMean)
index
pValue = index/1001*2
?hist
?qnorm
qnorm(0.95)
qnorm(0.975)
?qt
??ggplot
library(ggplot2)
?facet_wrap
getwd()
setwd("~/Data Science/Project1/cycle1-7")
library("survey")
library("dplyr")
library("data.table")
library("ggplot2")
library("choroplethr")
library("choroplethrMaps")
install.packages("survey")
install.packages("data.table")
install.packages("data.table")
install.packages("choroplethr")
install.packages("shoroplethrMaps")
library("survey")
library("dplyr")
library("data.table")
library("ggplot2")
library("choroplethr")
library("choroplethrMaps")
install.packages("survey")
>>>>>>> 6fc90004dedf9ec26abc7ef4ce5bd7ba8c55ea5d
